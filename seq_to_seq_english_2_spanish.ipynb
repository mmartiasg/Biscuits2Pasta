{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Transformers.Encoder import TransformerEncoder\n",
    "from Transformers.Decoder import TransformerDecoder\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load medical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./spa-eng/\"):\n",
    "    !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "    !unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pair = []\n",
    "\n",
    "with open(\"./spa-eng/spa.txt\") as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    for line in lines:\n",
    "        english, spanish = line.split(\"\\t\")\n",
    "        spanish = \"[start] \"+spanish+\" [end]\"\n",
    "        text_pair.append((english, spanish))\n",
    "\n",
    "# text_pair = np.array(text_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hug me.', '[start] AbrÃ¡zame. [end]')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pair[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "\n",
    "np.random.shuffle(text_pair)\n",
    "\n",
    "Train_N = math.ceil(len(text_pair)*0.70)\n",
    "Val_N = math.ceil(len(text_pair)*0.15)\n",
    "\n",
    "Train_pair = text_pair[:Train_N]\n",
    "Val_pairs = text_pair[Train_N:Train_N+Val_N]\n",
    "Test_pairs = text_pair[Train_N+Val_N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 83275 - val samples: 17845 - test samples: 17844\n"
     ]
    }
   ],
   "source": [
    "print(f\"train samples: {len(Train_pair)} - val samples: {len(Val_pairs)} - test samples: {len(Test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"This isn't what I ordered.\", '[start] Esto no es lo que he pedido. [end]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_pair[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "MAX_TOKENS = 15000\n",
    "MAX_SEQ = 20\n",
    "\n",
    "EMBEDDING_SIZE = 256\n",
    "LATENT_DIM = 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "Seq to seq model \n",
    "\n",
    "Source -> Target\n",
    "\n",
    "Source == Target autoregressive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:18:43.611991: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-27 15:18:43.612138: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_return_state_simple = tf.keras.layers.LSTM(units=5, return_state=True)\n",
    "lstm_return_state_bidierectional_merge = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=5, return_state=True), merge_mode=\"sum\")\n",
    "lstm_return_state_bidierectional_non_merge = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=5, return_state=True))\n",
    "lstm_return_state_bidierectional_merge_non_state = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=5))\n",
    "\n",
    "lstm = tf.keras.layers.LSTM(units=5, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleinput = np.expand_dims(np.expand_dims(np.array([1.0, 2.0, 3.0, 4.0]), axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[-0.00684281,  0.00288083, -0.09435974,  0.21446483,  0.3521424 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[-0.00684281,  0.00288083, -0.09435974,  0.21446483,  0.3521424 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[-0.00780093,  0.09245829, -0.20145708,  0.6661664 ,  0.38452134]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_return_state_simple(sampleinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 1.1151586 , -0.25592396, -0.05532645, -0.05811055,  0.36263284]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.7551459 , -0.13648446, -0.09073894, -0.0782069 ,  0.8852317 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.7551459 , -0.13648446, -0.09073894, -0.0782069 ,  0.8852317 ]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_return_state_bidierectional_merge(sampleinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       " array([[ 0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642,\n",
       "          0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.7551459 , -0.13648446, -0.09073894, -0.0782069 ,  0.8852317 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       " array([[ 0.7551459 , -0.13648446, -0.09073894, -0.0782069 ,  0.8852317 ]],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_return_state_bidierectional_non_merge(sampleinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[ 0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642,\n",
       "         0.5575793 , -0.12796198, -0.02766323, -0.02905528,  0.18131642]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_return_state_bidierectional_merge_non_state(sampleinput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#input\n",
    "source = tf.keras.layers.Input(shape=(None, ), dtype=\"int64\", name=\"source\")\n",
    "\n",
    "# ENCODER START\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=MAX_TOKENS, output_dim=EMBEDDING_SIZE, mask_zero=True)(source)\n",
    "encoded_source = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units=LATENT_DIM), merge_mode=\"sum\")(embeddings)\n",
    "# encoded_source = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=LATENT_DIM), merge_mode=\"sum\")(embeddings)\n",
    "# A full sentence is reduce to the last state of a LSTM\n",
    "# ENCODER END\n",
    "\n",
    "# DECODER\n",
    "# Encodded token and new generated token as inputs\n",
    "# Reverse process like the conv1DTranspose with the segmentation model\n",
    "target = tf.keras.layers.Input(shape=(None, ), dtype=\"int64\", name=\"target\")\n",
    "\n",
    "#learn a representation from the target (Vocabsize, latent_dim)\n",
    "#Should this be MAX_TOKENS+1 for the END?\n",
    "latent_space = tf.keras.layers.Embedding(input_dim=MAX_TOKENS, output_dim=EMBEDDING_SIZE, mask_zero=True)(target)\n",
    "\n",
    "#This will return (TARGET_SHAPE, LATENT_DIM)\n",
    "# decoder = tf.keras.layers.LSTM(units=LATENT_DIM, return_sequences=True)\n",
    "decoder = tf.keras.layers.GRU(units=LATENT_DIM, return_sequences=True)\n",
    "\n",
    "#TODO: FIX TO WORK WITH LSTM\n",
    "# decoded_sentence = decoder(latent_space, initial_state=[encoded_source[1], encoded_source[2]])\n",
    "# decoded_sentence = decoder(latent_space, initial_state=[encoded_source[0][0], encoded_source[0][1]])\n",
    "decoded_sentence = decoder(latent_space, initial_state=encoded_source)\n",
    "\n",
    "decoded_sentence = tf.keras.layers.Dropout(0.5)(decoded_sentence)\n",
    "\n",
    "target_next_step = tf.keras.layers.Dense(units=MAX_TOKENS, activation=\"softmax\")(decoded_sentence)\n",
    "# DECODER\n",
    "\n",
    "# CREATE MODEL ENCODER / DECODER\n",
    "encoder_decoder_model = tf.keras.Model(inputs=[source, target], outputs=target_next_step)\n",
    "\n",
    "# encoder_decoder_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-5), metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "encoder_decoder_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 256)    3840000     ['source[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 256)    3840000     ['target[0][0]']                 \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 1024)         7876608     ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, None, 1024)   3938304     ['embedding_1[0][0]',            \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 1024)   0           ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 15000)  15375000    ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,869,912\n",
      "Trainable params: 34,869,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def standarization_source(input_string):\n",
    "    strip_chars = string.punctuation\n",
    "    \n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    remove_puntuation = tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "    # remove_puntuation = tf.strings.regex_replace(remove_puntuation, \"[0-9]+[.,]?[0-9]*\", \"[NUMBER]\")\n",
    "    # remove_puntuation = tf.strings.regex_replace(remove_puntuation, \"\\[NUMBER]%\", \"[NUMBER_PERCENTAGE]\")\n",
    "    # remove_puntuation = tf.strings.regex_replace(remove_puntuation, \"\\-+\", \"\")\n",
    "\n",
    "    return remove_puntuation\n",
    "\n",
    "def standarization_target(input_string):\n",
    "    strip_chars = string.punctuation + \"Â¿\"\n",
    "    strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "    strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    remove_puntuation = tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "    # remove_puntuation = tf.strings.regex_replace(remove_puntuation, \"[0-9]+[.,]?[0-9]*\", \"[NUMBER]\")\n",
    "    # remove_puntuation = tf.strings.regex_replace(remove_puntuation, \"\\[NUMBER]%\", \"[NUMBER_PERCENTAGE]\")\n",
    "    # remove_puntuation = tf.strings.regex_replace(remove_puntuation, \"\\-+\", \"\")\n",
    "\n",
    "    return remove_puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vectorizer = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, output_sequence_length=MAX_SEQ, output_mode=\"int\", standardize=standarization_source)\n",
    "# Target will have one more character since it we need to cut one token to let the model to guess the next one\n",
    "target_vectorizer = tf.keras.layers.TextVectorization(max_tokens=MAX_TOKENS, output_sequence_length=MAX_SEQ+1, output_mode=\"int\", standardize=standarization_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:18:46.336292: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-27 15:18:46.374218: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-27 15:18:48.834564: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "source_vectorizer.adapt([pair[0] for pair in Train_pair])\n",
    "target_vectorizer.adapt([pair[1] for pair in Train_pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12085"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vectorizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(source, target):\n",
    "    source_vect = source_vectorizer(source)\n",
    "    target_vect = target_vectorizer(target)\n",
    "\n",
    "    return(\n",
    "        {\n",
    "            \"source\":source_vect,\n",
    "            # Remove the [end] token\n",
    "            # Target and source are the same\n",
    "            \"target\":target_vect[:, :-1]\n",
    "        }\n",
    "        #ignore [start] token and gets up to the [end] token\n",
    "        # Is one token ahead of target\n",
    "        ,target_vect[:, 1:]\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(([pair[0] for pair in Train_pair], [pair[1] for pair in Train_pair])).batch(128).map(format_dataset, num_parallel_calls=8).shuffle(1024).prefetch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(([pair[0] for pair in Val_pairs], [pair[1] for pair in Val_pairs])).batch(256).map(format_dataset, num_parallel_calls=8).prefetch(8)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(([pair[0] for pair in Test_pairs], [pair[1] for pair in Test_pairs])).batch(256).map(format_dataset, num_parallel_calls=8).prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocabulary = target_vectorizer.get_vocabulary()\n",
    "source_vocabulary = source_vectorizer.get_vocabulary()\n",
    "\n",
    "# Use to decode the predicted next token\n",
    "target_ditionary_vocabulary = dict(zip(range(MAX_TOKENS), target_vocabulary))\n",
    "source_ditionary_vocabulary = dict(zip(range(MAX_TOKENS), source_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_seq_example =datapoint[0][\"source\"][0]\n",
    "\n",
    "target_seq_example = datapoint[0][\"target\"][0]\n",
    "\n",
    "target_next_token = datapoint[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love tapioca pudding                '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([source_vocabulary[token.numpy()] for token in source_seq_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] me encanta el [UNK] de tapioca [end]            '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([target_ditionary_vocabulary[token.numpy()] for token in target_seq_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me encanta el [UNK] de tapioca [end]             '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([target_ditionary_vocabulary[token.numpy()] for token in target_next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs source shape: (128, 20)\n",
      "inputs target shape: (128, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_dataset.take(1):\n",
    "    print(f\"inputs source shape: {inputs['source'].shape}\")\n",
    "    print(f\"inputs target shape: {inputs['target'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\", filepath=\"./model/translation_eng_sp_rnn\")\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/translation_eng_sp_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/translation_eng_sp_rnn/assets\n"
     ]
    }
   ],
   "source": [
    "encoder_decoder_model.save(\"./model/translation_eng_sp_rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:19:04.058021: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.096625: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.113794: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.121856: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.239774: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.247687: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.317301: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.386884: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.452525: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.513026: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.520889: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.528371: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.536003: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.933658: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:05.940993: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:06.369354: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:06.852656: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.179589: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.187422: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.195833: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.297265: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.304174: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.845899: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.853328: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.889873: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.901921: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.910264: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.958673: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.966597: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:07.974649: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.042730: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.297371: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.308799: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.315419: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.322563: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.329272: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.547071: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.554110: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.628591: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.635947: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.660656: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond/while' has 13 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n",
      "2023-05-27 15:19:08.668357: W tensorflow/core/common_runtime/graph_constructor.cc:805] Node 'cond' has 4 outputs but the _output_shapes attribute specifies shapes for 46 outputs. Output shapes may be inaccurate.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    encoder_decoder_model = tf.keras.models.load_model(\"./model/translation_eng_sp_rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocabulary = target_vectorizer.get_vocabulary()\n",
    "# Use to decode the predicted next token\n",
    "ditionary_vocabulary = dict(zip(range(len(target_vocabulary)), target_vocabulary))\n",
    "\n",
    "def decode_sequence(input_sequence):\n",
    "\n",
    "    # Initial tokens\n",
    "    tokenized_input_sentence = source_vectorizer([input_sequence])[:, :-1]\n",
    "\n",
    "    # We start with start token\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(MAX_SEQ):\n",
    "        tokenized_target_sentence = target_vectorizer([decoded_sentence])\n",
    "\n",
    "        next_token_predictions = encoder_decoder_model.predict([\n",
    "                                        tokenized_input_sentence, \n",
    "                                        tokenized_target_sentence\n",
    "                                ],\n",
    "                                verbose=0)\n",
    "        \n",
    "        # the output is [1, MAX_SEQ+1, MAX_TOKENS]\n",
    "        #This will give 1 value for [1, MAX_SEQ, 1] <- next token index decoded from the softmax\n",
    "        sample_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "\n",
    "        # find token for that decoded token index\n",
    "        sample_token = ditionary_vocabulary[sample_token_index]\n",
    "\n",
    "        #Append the new generated token\n",
    "        decoded_sentence += \" \"+sample_token\n",
    "\n",
    "        #Finish before running out of tokens in the max sequence if we found the [END] token\n",
    "        if sample_token == \"[end]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Even a broken clock is right twice a day.',\n",
       " '[start] Incluso un reloj roto estÃ¡ a la hora dos veces al dÃ­a. [end]')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] envuÃ©lvalo dotaciÃ³n suegras ordenada vigilaba intentarÃ­a batidos luce queja poyang secundarios Ã©tico entrega roja sentarÃ¡ lealtad chapoteo publicado placentero picÃ³'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoded = decode_sequence(\"You must take care of your dog yourself.\")\n",
    "\n",
    "cpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] envuÃ©lvalo precavido idiomas zapatillas seriamente artistas consideraba umbral valla ofendido caminÃ³ llevara avergonzada seguridad resfriada argumento acatar respiratorios torciÃ³ volvÃ­an'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoded = decode_sequence(\"You must take care of your cat yourself.\")\n",
    "\n",
    "cpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] envuÃ©lvalo precavido idiomas zapatillas seriamente artistas consideraba umbral valla ofendido caminÃ³ llevara avergonzada seguridad resfriada argumento acatar respiratorios torciÃ³ volvÃ­an'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoded = decode_sequence(\"You must take play with your cat.\")\n",
    "\n",
    "cpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] sÃºbete hola morderse escondida tomarÃ© considero cambiara mintiÃ³ tiza propone ecuador envÃ­eme vÃ­spera remera bolso adivinen motor mil golfista alquilamos'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoded = decode_sequence(\"You must take play with your dog.\")\n",
    "\n",
    "cpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] Â¡estamos Â¡estamos tocase pertenecen constantemente sonreÃ­ste tubo olvidaste excitante visitÃ© resulten continuarÃ¡ medida abarrotado vente lejano Â¡levantate Â«Â¡protestoÂ» tic robÃ©'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoded = decode_sequence(\"You must go to the doctor first thing in the morning!\")\n",
    "\n",
    "cpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:19:15.440640: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-27 15:19:15.906133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-27 15:19:15.963519: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-27 15:19:16.129095: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[start] trescientos cinturones cinturones supieron supieron evento express golpeÃ© saliendo decorar radiador radiador radiador radiador satisfactoria transbordador despejarse dominio suponer suponer'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    gpu_decoded = decode_sequence(\"You must go to the doctor first thing in the morning!\")\n",
    "\n",
    "gpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] prefiero peinado quÃ­micos antigua perÃ­odo juega explÃ­came reemplazar Â¡despertate sentate relajes alumnos tonta retroceder tomaste vivÃ­ besara peluca trabajando transladar'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoded = decode_sequence(\"time is of the essence\")\n",
    "\n",
    "cpu_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[start] trescientos cinturones cinturones supieron supieron evento express golpeÃ© saliendo decorar radiador radiador radiador radiador satisfactoria transbordador despejarse dominio suponer suponer'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    gpu_decoded = decode_sequence(\"You must go to the doctor first thing in the morning!\")\n",
    "\n",
    "gpu_decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, dim_emb, dim_dense, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_emb = dim_emb\n",
    "        self.dim_dense = dim_dense\n",
    "\n",
    "        #OUTPUT concatenate from each space (head) for the proyections of q, k, v weighted by their attention scores.\n",
    "        # q (embeddings) -> dense\n",
    "        # v (embeddings) -> dense\n",
    "        # k (embeddings) -> dense\n",
    "        # attention score over k and v\n",
    "        # [Num heads, dim_dense, dim_dense, dim_dense]\n",
    "        self.mha_1 = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.dim_emb, value_dim=self.dim_dense)\n",
    "        self.mha_2 = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.dim_emb, value_dim=self.dim_dense)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.proy = tf.keras.Sequential([tf.keras.layers.Dense(units=self.dim_dense, activation=\"relu\"), tf.keras.layers.Dense(units=self.dim_emb)])\n",
    "\n",
    "    def get_casual_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_lenght = input_shape[0], input_shape[1]\n",
    "\n",
    "        # Add a new dim to the range this will be \n",
    "        # [Sequence_lenght, 1]\n",
    "        i = tf.range(sequence_lenght)[:, tf.newaxis]\n",
    "        # [sequence_lenght,]\n",
    "        j = tf.range(sequence_lenght)\n",
    "\n",
    "        mask = tf.cast(i>=j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "                #Add a dimension [batch size, 1]\n",
    "                [tf.expand_dims(batch_size, -1),\n",
    "                tf.constant([1,1], dtype=tf.int32)],\n",
    "                axis=0\n",
    "        )\n",
    "\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, source_encoded, target_input, mask=None):\n",
    "        # set casual attention mask\n",
    "        casual_mask = self.get_casual_attention_mask(target_input)\n",
    "        \n",
    "        padding_mask = None\n",
    "        # Set the mask for when I need to ignore the tokens used to pad the sentence\n",
    "        \n",
    "        if mask is not None:\n",
    "            #Why this new axis?\n",
    "            #Reduce each sequence to a single vector for classification via a global pooling layer\n",
    "            #Still do not get it :C TODO: review\n",
    "            # I think the first is batch, second will be occupy by the sentence and last by dimensions embeddings\n",
    "            # The mask I get is not complete then?\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, casual_mask)\n",
    "\n",
    "        attention_1_output = self.mha_1(query=target_input, key=target_input, value=target_input, attention_mask=casual_mask)\n",
    "\n",
    "        nomr_1_output = self.norm_1(target_input + attention_1_output)\n",
    "\n",
    "        attention_2_output = self.mha_2(query=nomr_1_output, key=source_encoded, value=source_encoded, attention_mask=padding_mask)\n",
    "\n",
    "        norm_2_output = self.norm_2(attention_2_output+attention_1_output)\n",
    "\n",
    "        proy = self.proy(norm_2_output)\n",
    "\n",
    "        nomr_3_output = self.norm_3(proy + norm_2_output)\n",
    "\n",
    "        return nomr_3_output\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\" \n",
    "            This is a dictionary with the parameter's values to reinstantiate the layer when the model is loaded\n",
    "        \"\"\"\n",
    "        return {\"num_heads\": self.num_heads,\n",
    "                \"dim_emb\": self.dim_emb,\n",
    "                \"dim_dense\":self.dim_dense}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causa attention mask\n",
    "\n",
    "- Why do we need this??\n",
    "- How does this works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = source_vectorizer(\"That is to ignore padding values\")[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 20])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "array([[  12,    8,    4, 1872,    1, 7979,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Batch size, sentence sequence lenght]\n",
    "input_shape = tf.shape(inputs)\n",
    "\n",
    "batch_size, sequence_lenght = input_shape[0], input_shape[1]\n",
    "\n",
    "# Add a new dim to the range this will be [Sequence_lenght, 1]\n",
    "# Creates a new tensor with shape\n",
    "\n",
    "# [Sequence max lenght, 1]\n",
    "i = tf.range(sequence_lenght)[:, tf.newaxis]\n",
    "\n",
    "# [sequence_lenght,]\n",
    "j = tf.range(sequence_lenght)\n",
    "\n",
    "# this builds an lower diagonal matrix\n",
    "# it will be false for i>=j\n",
    "#    j=0       j=1\n",
    "# i=0 1         0          00000\n",
    "# i=1 1(i>j)    1(i==j)    00000\n",
    "\n",
    "# it will be a [max_seq, max_seq] matrix\n",
    "mask = tf.cast(i>=j, dtype=\"int32\")\n",
    "\n",
    "#Reshape the mask to [1, max_seq, max_seq]\n",
    "mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "\n",
    "# concat on axis=0 means by row it will be the vector [batch_size, 1, 1] of shape (3,) \n",
    "mult = tf.concat(\n",
    "        # it creates a vector of shape [1, 1] -> [batch_size]\n",
    "        [tf.expand_dims(batch_size, -1),\n",
    "         # vector of size [1, 1] value [1, 1]\n",
    "        tf.constant([1,1], dtype=tf.int32)],\n",
    "        axis=0\n",
    ")\n",
    "\n",
    "# So mask is shape [20, 20] and mult is shape (3,) and its value is [batch_size, 1, 1] with 1 sentence in the batch [1, 1, 1]\n",
    "\n",
    "#Mult is used to propagate the mask, wich is a [] to the whole BATCH\n",
    "casual_mask = tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 20), dtype=int32, numpy=\n",
       "array([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casual_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 20, 20])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([20, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(sequence_lenght)[:, tf.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1, 2], [3, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 5), dtype=int32, numpy=\n",
       "array([[[2, 1, 1, 1, 2],\n",
       "        [2, 0, 0, 0, 2]]], dtype=int32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(tf.constant([[[2, 1, 1, 1, 2], [2, 0, 0, 0, 2]]]), mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 20), dtype=int32, numpy=\n",
       "array([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6],\n",
       "       [7],\n",
       "       [8],\n",
       "       [9]], dtype=int32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)[:, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 1, 1, 1, 1], dtype=int32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.range(5)>=tf.range(5), dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.tile?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from Transformers.PositionalEncoding import BasicPositionalEmbeddings\n",
    "\n",
    "num_heads = 1\n",
    "DENSE_DIM = 256\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(None, ), dtype=\"int64\", name=\"source\")\n",
    "\n",
    "e = BasicPositionalEmbeddings(dim_emb=EMBEDDING_SIZE, max_tokens=MAX_TOKENS, max_seq_length=MAX_SEQ)(encoder_inputs)\n",
    "\n",
    "encoder_outputs = TransformerEncoder(num_heads=num_heads, dim_emb=EMBEDDING_SIZE, dim_dense=DENSE_DIM)(e)\n",
    "\n",
    "decoder_inputs = tf.keras.Input(shape=(None, ), dtype=\"int64\", name=\"target\")\n",
    "\n",
    "x = BasicPositionalEmbeddings(dim_emb=EMBEDDING_SIZE, max_tokens=MAX_TOKENS, max_seq_length=MAX_SEQ)(decoder_inputs)\n",
    "\n",
    "#Source, target\n",
    "ex = TransformerDecoder(num_heads=num_heads, dim_dense=DENSE_DIM, dim_emb=EMBEDDING_SIZE)(encoder_outputs, x)\n",
    "\n",
    "ex = tf.keras.layers.Dropout(0.5)(ex)\n",
    "\n",
    "next_token = tf.keras.layers.Dense(MAX_TOKENS, activation=\"softmax\")(ex)\n",
    "\n",
    "transformer_eng_spa_model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=next_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_eng_spa_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " basic_positional_embeddings (B  (None, None, 256)   3845120     ['source[0][0]']                 \n",
      " asicPositionalEmbeddings)                                                                        \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   395776      ['basic_positional_embeddings[0][\n",
      " erEncoder)                                                      0]']                             \n",
      "                                                                                                  \n",
      " basic_positional_embeddings_1   (None, None, 256)   3845120     ['target[0][0]']                 \n",
      " (BasicPositionalEmbeddings)                                                                      \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   659456      ['transformer_encoder[0][0]',    \n",
      " erDecoder)                                                       'basic_positional_embeddings_1[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 15000)  3855000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,600,472\n",
      "Trainable params: 12,600,472\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_eng_spa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:19:21.426175: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651/651 [==============================] - ETA: 0s - loss: 1.9488 - accuracy: 0.7326"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:20:58.724567: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/translation_eng_sp_transformer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/translation_eng_sp_transformer/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651/651 [==============================] - 109s 164ms/step - loss: 1.9488 - accuracy: 0.7326 - val_loss: 1.4314 - val_accuracy: 0.7848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiasgonzalez/miniforge3/envs/tf_2_9_metal_0_5_keras/lib/python3.10/site-packages/keras/engine/functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/Users/matiasgonzalez/miniforge3/envs/tf_2_9_metal_0_5_keras/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e0780c40>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\", filepath=\"./model/translation_eng_sp_transformer\")\n",
    "early_callback = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\")\n",
    "\n",
    "transformer_eng_spa_model.fit(train_dataset, validation_data=val_dataset, epochs=1, callbacks=[early_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fuckingframework/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fuckingframework/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2e440e200>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.Sequential([tf.keras.Input(shape=(2,)), tf.keras.layers.Dense(1)]).save(\"fuckingframework\")\n",
    "\n",
    "tf.keras.models.load_model(\"fuckingframework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: stupidoriginalmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: stupidoriginalmodel/assets\n",
      "/Users/matiasgonzalez/miniforge3/envs/tf_2_9_metal_0_5_keras/lib/python3.10/site-packages/keras/engine/functional.py:1384: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/Users/matiasgonzalez/miniforge3/envs/tf_2_9_metal_0_5_keras/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"stupidoriginalmodel\"\n",
    "\n",
    "transformer_eng_spa_model.save(MODEL_PATH)\n",
    "stupid_model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " basic_positional_embeddings (B  (None, None, 256)   3845120     ['source[0][0]']                 \n",
      " asicPositionalEmbeddings)                                                                        \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   395776      ['basic_positional_embeddings[0][\n",
      " erEncoder)                                                      0]']                             \n",
      "                                                                                                  \n",
      " basic_positional_embeddings_1   (None, None, 256)   3845120     ['target[0][0]']                 \n",
      " (BasicPositionalEmbeddings)                                                                      \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   659456      ['transformer_encoder[0][0]',    \n",
      " erDecoder)                                                       'basic_positional_embeddings_1[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 15000)  3855000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,600,472\n",
      "Trainable params: 12,600,472\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "stupid_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocabulary = target_vectorizer.get_vocabulary()\n",
    "# Use to decode the predicted next token\n",
    "ditionary_vocabulary = dict(zip(range(len(target_vocabulary)), target_vocabulary))\n",
    "\n",
    "def decode_sequence_transformer(input_sequence):\n",
    "    # Initial tokens\n",
    "    tokenized_input_sentence = source_vectorizer([input_sequence])\n",
    "\n",
    "    # We start with start token\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(MAX_SEQ):\n",
    "        # tokenized_target_sentence = target_vectorizer([decoded_sentence])\n",
    "        tokenized_target_sentence = target_vectorizer([decoded_sentence])[:, :-1]\n",
    "\n",
    "        next_token_predictions = stupid_model.predict([\n",
    "                                        tokenized_input_sentence, \n",
    "                                        tokenized_target_sentence\n",
    "                                ],\n",
    "                                verbose=0)\n",
    "        \n",
    "        # the output is [1, MAX_SEQ+1, MAX_TOKENS]\n",
    "        #This will give 1 value for [1, MAX_SEQ, 1] <- next token index decoded from the softmax\n",
    "        sample_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "\n",
    "        # find token for that decoded token index\n",
    "        sample_token = ditionary_vocabulary[sample_token_index]\n",
    "\n",
    "        #Append the new generated token\n",
    "        decoded_sentence += \" \"+sample_token\n",
    "\n",
    "        #Finish before running out of tokens in the max sequence if we found the [END] token\n",
    "        if sample_token == \"[end]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:22:58.513383: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] no debe de gusta tu perro [end]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    print(decode_sequence_transformer(\"You must take care of your dog yourself.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 15:21:13.040557: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[start] tienes de le gusta tu perro [end]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    print(decode_sequence_transformer(\"You must take care of your dog yourself.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  (\"I'm married.\", '[start] Soy casado. [end]')\n",
      "Decoded:  [start] mama parÃ­s velador cerrara tulipanes acompaÃ±ar desafiÃ³ aparcamiento guerra urbano alimentar agencia tratarme darme terminÃ¡ndolo emocionado comparamos sospechan ambigÃ¼edades recordarÃ©\n",
      "Source:  ('Even a broken clock is right twice a day.', '[start] Incluso un reloj roto estÃ¡ a la hora dos veces al dÃ­a. [end]')\n",
      "Decoded:  [start] autobuses resistirme quitaste rezar colapsar relajes mÃ­a pequeÃ±a limones prestes votÃ© encontraron amplia cortesÃ­a confiable descubierto encarguÃ© semana altibajos barbaridad\n",
      "Source:  ('She kept him waiting for a long time.', '[start] Le hizo esperar un buen rato. [end]')\n",
      "Decoded:  [start] prefiero peinado quÃ­micos antigua perÃ­odo juega explÃ­came reemplazar Â¡despertate sentate relajes alumnos tonta retroceder tomaste vivÃ­ besara peluca trabajando transladar\n",
      "Source:  ('You are very elegant.', '[start] EstÃ¡s muy elegante. [end]')\n",
      "Decoded:  [start] mangas ufff usarÃ© 600 reemplazarÃ¡ creÃ­amos averiguarÃ© vaga asistieron observado historia cientÃ­ficos tabÃº encontrarÃ¡n tumbada amenaza reventaron estupefactos rizado fueron\n",
      "Source:  ('Can people change?', '[start] Â¿Puede la gente cambiar? [end]')\n",
      "Decoded:  [start] acercando mismos sonriera recordaran cinta trÃ¡igale probaste dinosaurios leÃ­a sobresaliente reflejo jugaba guapos coinciden consiguiera sonrÃ­o esperen revueltos reunirÃ­an cantase\n",
      "Source:  (\"It's difficult to help people who don't believe they need help.\", '[start] Es difÃ­cil ayudar a las personas que no creen necesitar ayuda. [end]')\n",
      "Decoded:  [start] envuÃ©lvalo mÃ­as mancha finos distinguirlo recuperar caÃ­da mezclar siguiera revisÃ© pusiera saldremos querido huesos deprimido mudarse granjeros usted salvarle cometa\n",
      "Source:  ('He is not aggressive enough to succeed in business.', '[start] Ãl no es lo suficientemente agresivo para triunfar en los negocios. [end]')\n",
      "Decoded:  [start] prefiero rezo 100 colores duramente inaceptable Â¡eso explico jubilarme margen ponerle automovilÃ­sticos imitaciÃ³n corridas raspÃ³ raspÃ³ detesto patinaje desagradables pobladores\n",
      "Source:  (\"Tom doesn't think it's such a big deal.\", '[start] Tom no cree que eso sea la gran cosa. [end]')\n",
      "Decoded:  [start] gramÃ¡tica quitaste olÃ­a beba mintiendo sirviÃ³ secreta alaska banquero rezaba b reserva liberaron Â¡dese Ã¡cida quites funcionarÃ¡ estuvieron corbatas salchichas\n",
      "Source:  ('If you are not firm with the children, they will get out of hand.', '[start] Si no eres firme con los niÃ±os, se te irÃ¡n de las manos. [end]')\n",
      "Decoded:  [start] rodica charles harto vacuna plenamente piensa gol alto sentÃ© tecnologÃ­as comerme restringida deberÃ­ais Â¡tira deberÃ­ais encima oigas querÃ­an sentando electrones\n",
      "Source:  ('Love of money is the root of all evil.', '[start] El amor al dinero es la raÃ­z de todos los males. [end]')\n",
      "Decoded:  [start] reconquistarla cuando ayudarnos duerma farola regiÃ³n plan culpabilidad cedÃ­ lloro ejercicio tuyo trigo abraza rancherÃ­a descompuso eua durmieron hijos rige\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    for i in range(10):\n",
    "        print(\"Source: \", Test_pairs[i])\n",
    "        print(\"Decoded: \", decode_sequence(Test_pairs[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  (\"I'm married.\", '[start] Soy casado. [end]')\n",
      "Decoded:  [start] estoy [UNK] [end]\n",
      "Source:  ('Even a broken clock is right twice a day.', '[start] Incluso un reloj roto estÃ¡ a la hora dos veces al dÃ­a. [end]')\n",
      "Decoded:  [start] un perro de perro es por dos por por dÃ­a [end]\n",
      "Source:  ('She kept him waiting for a long time.', '[start] Le hizo esperar un buen rato. [end]')\n",
      "Decoded:  [start] ella le le que vez por una tiempo [end]\n",
      "Source:  ('You are very elegant.', '[start] EstÃ¡s muy elegante. [end]')\n",
      "Decoded:  [start] eres muy [end]\n",
      "Source:  ('Can people change?', '[start] Â¿Puede la gente cambiar? [end]')\n",
      "Decoded:  [start] puede gente de nada [end]\n",
      "Source:  (\"It's difficult to help people who don't believe they need help.\", '[start] Es difÃ­cil ayudar a las personas que no creen necesitar ayuda. [end]')\n",
      "Decoded:  [start] es que que gente que no no que no querÃ­a hacer hacer hacer [end]\n",
      "Source:  ('He is not aggressive enough to succeed in business.', '[start] Ãl no es lo suficientemente agresivo para triunfar en los negocios. [end]')\n",
      "Decoded:  [start] Ãl no es de es [end]\n",
      "Source:  (\"Tom doesn't think it's such a big deal.\", '[start] Tom no cree que eso sea la gran cosa. [end]')\n",
      "Decoded:  [start] tom no es es es un es [end]\n",
      "Source:  ('If you are not firm with the children, they will get out of hand.', '[start] Si no eres firme con los niÃ±os, se te irÃ¡n de las manos. [end]')\n",
      "Decoded:  [start] si no estÃ¡s [UNK] con la niÃ±os de los niÃ±os [end]\n",
      "Source:  ('Love of money is the root of all evil.', '[start] El amor al dinero es la raÃ­z de todos los males. [end]')\n",
      "Decoded:  [start] los gusta dinero es el dinero de la mundo [end]\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    for i in range(10):\n",
    "        print(\"Source: \", Test_pairs[i])\n",
    "        print(\"Decoded: \", decode_sequence_transformer(Test_pairs[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decoded_sentences(sentence, decode_sequence, decode_sequence_transformer):\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        print(\"GRU encoder-decoder\")\n",
    "        print(decode_sequence(sentence))\n",
    "        print(\"Transformer encoder-decoder\")\n",
    "        print(decode_sequence_transformer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU encoder-decoder\n",
      "[start] envuÃ©lvalo precavido idiomas zapatillas seriamente artistas consideraba umbral valla ofendido caminÃ³ llevara avergonzada seguridad resfriada argumento acatar respiratorios torciÃ³ volvÃ­an\n",
      "Transformer encoder-decoder\n",
      "[start] tienes de [UNK] con tu gato [end]\n"
     ]
    }
   ],
   "source": [
    "print_decoded_sentences(\"You must take play with your cat.\", decode_sequence, decode_sequence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU encoder-decoder\n",
      "[start] envuÃ©lvalo dotaciÃ³n suegras ordenada vigilaba intentarÃ­a batidos luce queja poyang secundarios Ã©tico entrega roja sentarÃ¡ lealtad chapoteo publicado placentero picÃ³\n",
      "Transformer encoder-decoder\n",
      "[start] tienes de le gusta tu perro [end]\n"
     ]
    }
   ],
   "source": [
    "print_decoded_sentences(\"You must take care of your dog yourself.\", decode_sequence, decode_sequence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU encoder-decoder\n",
      "[start] envuÃ©lvalo precavido idiomas zapatillas seriamente artistas consideraba umbral valla ofendido caminÃ³ llevara avergonzada seguridad resfriada argumento acatar respiratorios torciÃ³ volvÃ­an\n",
      "Transformer encoder-decoder\n",
      "[start] tienes de me gusta tu gato [end]\n"
     ]
    }
   ],
   "source": [
    "print_decoded_sentences(\"You must take care of your cat yourself.\", decode_sequence, decode_sequence_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
